{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMo410nY3vv0XJF1xFUhgbm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhayesn13/Test/blob/main/Working_Crawler_with_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNFegWNalyc-"
      },
      "outputs": [],
      "source": [
        "#Working Code\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import threading\n",
        "import re\n",
        "\n",
        "# Create webcrawler class\n",
        "class WebCrawler:\n",
        "    def __init__(self, start_url, visiting_strategy='preorder'):\n",
        "        self.start_url = start_url\n",
        "        self.visiting_strategy = visiting_strategy.lower()\n",
        "        self.visited_urls = set()\n",
        "        self.corpus = {}\n",
        "        self.main_domain = urlparse(start_url).netloc\n",
        "        self.lock = threading.Lock()  # Lock for thread-safe access to shared data\n",
        "\n",
        "    def crawl(self, url):\n",
        "        if url not in self.visited_urls and self.is_same_domain(url):\n",
        "            print(f\"Visiting: {url}\")\n",
        "            self.visited_urls.add(url)\n",
        "            try:\n",
        "                response = requests.get(url)\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "                title = soup.title.string.strip() if soup.title else 'Untitled'\n",
        "                text_content = self.extract_text_content(soup)\n",
        "\n",
        "                with self.lock:  # Thread-safe update of shared data\n",
        "                    self.corpus[url] = text_content\n",
        "\n",
        "                print(f\"Text Content: {text_content[:100]}...\")  # Output a snippet of text\n",
        "\n",
        "                if self.visiting_strategy == 'preorder':\n",
        "                    links = self.extract_links(soup)\n",
        "                    threads = []\n",
        "                    for link in links:\n",
        "                        thread = threading.Thread(target=self.crawl, args=(link,))\n",
        "                        threads.append(thread)\n",
        "                        thread.start()\n",
        "\n",
        "                    # Wait for all threads to complete\n",
        "                    for thread in threads:\n",
        "                        thread.join()\n",
        "\n",
        "                # Additional visiting strategies (inorder, postorder) can be implemented here\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error crawling {url}: {e}\")\n",
        "\n",
        "    def extract_text_content(self, soup):\n",
        "        # Extract text content only from the body of the HTML\n",
        "        text_content = ' '.join([p.get_text(separator=' ', strip=True) for p in soup.body.find_all('p')])\n",
        "        return text_content\n",
        "\n",
        "    def extract_links(self, soup):\n",
        "        # Extract all links from the page\n",
        "        links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
        "        # Filter internal links only\n",
        "        links = [urljoin(self.start_url, link) for link in links if link.startswith(('http', 'https'))]\n",
        "        # Exclude PDF links\n",
        "        links = [link for link in links if not link.endswith('.pdf')]\n",
        "        # Filter out external links\n",
        "        links = [link for link in links if self.is_same_domain(link)]\n",
        "        # Exclude links with 'resources' in the URL\n",
        "        links = [link for link in links if 'resources' not in link.lower()]\n",
        "        return links\n",
        "\n",
        "    def is_same_domain(self, url):\n",
        "        return urlparse(url).netloc == self.main_domain\n",
        "\n",
        "    def start_crawling(self):\n",
        "        self.crawl(self.start_url)\n",
        "\n",
        "    def get_crawled_data(self):\n",
        "        return self.corpus\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Get the starting URL from the user\n",
        "    start_url = input(\"Enter the website's URL: \")\n",
        "\n",
        "    # Instantiate the WebCrawler with the provided URL and visiting strategy\n",
        "    crawler = WebCrawler(start_url=start_url, visiting_strategy='preorder')\n",
        "\n",
        "    # Start crawling\n",
        "    crawler.start_crawling()\n",
        "\n",
        "    # Get the crawled data\n",
        "    crawled_data = crawler.get_crawled_data()\n",
        "\n",
        "    # Print the crawled data\n",
        "    for url, content in crawled_data.items():\n",
        "        print(f\"URL: {url}\")\n",
        "        print(f\"Content: {content[:100]}...\")  # Print a snippet of content\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying content from a specific webpage\n",
        "\n",
        "# Assuming 'crawled_data' is the dictionary containing the crawled data\n",
        "desired_url = 'https://www.stjohns.edu/academics/programs/clinical-laboratory-sciences-bachelor-science'\n",
        "\n",
        "# Check if the URL is present in the crawled data\n",
        "if desired_url in crawled_data:\n",
        "    content = crawled_data[desired_url]\n",
        "    print(\"Content for URL:\")\n",
        "    print(content)\n",
        "else:\n",
        "    print(\"URL not found in crawled data.\")"
      ],
      "metadata": {
        "id": "lt_906sNl2uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Crawler with processed text. Includes tokens, removed stopwords, lemmatized tokens, stemming, and sentiment score\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import threading\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "class TextProcessor:\n",
        "    def __init__(self):\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('stopwords')\n",
        "        nltk.download('wordnet')\n",
        "        nltk.download('vader_lexicon')\n",
        "\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        cleaned_text = cleaned_text.lower()\n",
        "        return cleaned_text\n",
        "\n",
        "    def tokenize_text(self, text):\n",
        "        doc = self.nlp(text)\n",
        "        tokens = [token.text for token in doc]\n",
        "        return tokens\n",
        "\n",
        "    def remove_stopwords(self, tokens):\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in self.stop_words]\n",
        "        return filtered_tokens\n",
        "\n",
        "    def lemmatize_text(self, tokens):\n",
        "        doc = self.nlp(\" \".join(tokens))\n",
        "        lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "        return lemmatized_tokens\n",
        "\n",
        "    def stem_text(self, tokens):\n",
        "        stemmed_tokens = [self.stemmer.stem(token) for token in tokens]\n",
        "        return stemmed_tokens\n",
        "\n",
        "    def analyze_sentiment(self, text):\n",
        "        sentiment_scores = self.sentiment_analyzer.polarity_scores(text)\n",
        "        return sentiment_scores\n",
        "\n",
        "class WebCrawler:\n",
        "    def __init__(self, start_url, visiting_strategy='preorder'):\n",
        "        self.start_url = start_url\n",
        "        self.visiting_strategy = visiting_strategy.lower()\n",
        "        self.visited_urls = set()\n",
        "        self.corpus = {}\n",
        "        self.main_domain = urlparse(start_url).netloc\n",
        "        self.lock = threading.Lock()  # Lock for thread-safe access to shared data\n",
        "        self.text_processor = TextProcessor()  # Instantiate TextProcessor\n",
        "\n",
        "    def crawl(self, url):\n",
        "        if url not in self.visited_urls and self.is_same_domain(url):\n",
        "            print(f\"Visiting: {url}\")\n",
        "            self.visited_urls.add(url)\n",
        "            try:\n",
        "                response = requests.get(url)\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "                title = soup.title.string.strip() if soup.title else 'Untitled'\n",
        "                text_content = self.extract_text_content(soup)\n",
        "\n",
        "                # Process text content\n",
        "                cleaned_text = self.text_processor.clean_text(text_content)\n",
        "                tokens = self.text_processor.tokenize_text(cleaned_text)\n",
        "                filtered_tokens = self.text_processor.remove_stopwords(tokens)\n",
        "                lemmatized_tokens = self.text_processor.lemmatize_text(filtered_tokens)\n",
        "                stemmed_tokens = self.text_processor.stem_text(filtered_tokens)\n",
        "                sentiment_scores = self.text_processor.analyze_sentiment(text_content)\n",
        "\n",
        "                with self.lock:  # Thread-safe update of shared data\n",
        "                    self.corpus[url] = {\n",
        "                        'title': title,\n",
        "                        'text_content': text_content,\n",
        "                        'cleaned_text': cleaned_text,\n",
        "                        'tokens': tokens,\n",
        "                        'filtered_tokens': filtered_tokens,\n",
        "                        'lemmatized_tokens': lemmatized_tokens,\n",
        "                        'stemmed_tokens': stemmed_tokens,\n",
        "                        'sentiment_scores': sentiment_scores\n",
        "                    }\n",
        "\n",
        "                print(f\"Title: {title}\")\n",
        "\n",
        "                if self.visiting_strategy == 'preorder':\n",
        "                    links = self.extract_links(soup)\n",
        "                    threads = []\n",
        "                    for link in links:\n",
        "                        thread = threading.Thread(target=self.crawl, args=(link,))\n",
        "                        threads.append(thread)\n",
        "                        thread.start()\n",
        "\n",
        "                    # Wait for all threads to complete\n",
        "                    for thread in threads:\n",
        "                        thread.join()\n",
        "\n",
        "                # Additional visiting strategies (inorder, postorder) can be implemented here\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error crawling {url}: {e}\")\n",
        "\n",
        "    def extract_text_content(self, soup):\n",
        "        text_content = ' '.join([p.get_text(separator=' ', strip=True) for p in soup.body.find_all('p')])\n",
        "        return text_content\n",
        "\n",
        "    def extract_links(self, soup):\n",
        "        links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
        "        links = [urljoin(self.start_url, link) for link in links if link.startswith(('http', 'https'))]\n",
        "        links = [link for link in links if not link.endswith('.pdf')]\n",
        "        links = [link for link in links if self.is_same_domain(link)]\n",
        "        links = [link for link in links if 'resources' not in link.lower()]\n",
        "        return links\n",
        "\n",
        "    def is_same_domain(self, url):\n",
        "        return urlparse(url).netloc == self.main_domain\n",
        "\n",
        "    def start_crawling(self):\n",
        "        self.crawl(self.start_url)\n",
        "\n",
        "    def get_crawled_data(self):\n",
        "        return self.corpus\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Get the starting URL from the user\n",
        "    start_url = input(\"Enter the website's URL: \")\n",
        "\n",
        "    # Instantiate the WebCrawler with the provided URL and visiting strategy\n",
        "    crawler = WebCrawler(start_url=start_url, visiting_strategy='preorder')\n",
        "\n",
        "    # Start crawling\n",
        "    crawler.start_crawling()\n",
        "\n",
        "    # Get the crawled data\n",
        "    crawled_data = crawler.get_crawled_data()\n",
        "\n",
        "    # Print the crawled data\n",
        "    for url, data in crawled_data.items():\n",
        "        print(f\"URL: {url}\")\n",
        "        print(f\"Title: {data['title']}\")\n",
        "        print(f\"Cleaned Text: {data['cleaned_text']}\")\n",
        "        print(f\"Tokens: {data['tokens']}\")\n",
        "        print(f\"Filtered Tokens: {data['filtered_tokens']}\")\n",
        "        print(f\"Lemmatized Tokens: {data['lemmatized_tokens']}\")\n",
        "        print(f\"Stemmed Tokens: {data['stemmed_tokens']}\")\n",
        "        print(f\"Sentiment Scores: {data['sentiment_scores']}\")\n"
      ],
      "metadata": {
        "id": "rn0PDAHqmGIs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}